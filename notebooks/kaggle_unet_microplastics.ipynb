{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: U-Net Microplastic Segmentation - Kaggle Notebook\n",
    "\n",
    "**Complete pipeline for pixel-level microplastic segmentation**\n",
    "\n",
    "- **Task**: Binary segmentation (microplastic vs background)\n",
    "- **Model**: U-Net with Dice + BCE loss\n",
    "- **Dataset**: Microplastics from marine/ocean environments\n",
    "- **Framework**: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q albumentations opencv-python-headless scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup\n",
    "\n",
    "**You need to add your dataset to this Kaggle notebook:**\n",
    "\n",
    "1. Click \"+ Add data\" button on the right panel\n",
    "2. Search for: `microplastic` or your uploaded dataset name\n",
    "3. Click \"Add\"\n",
    "\n",
    "**Recommended datasets on Kaggle:**\n",
    "- `imtkaggleteam/microplastic-dataset-for-computer-vision`\n",
    "- Or upload your own DeepParticle dataset\n",
    "\n",
    "**Update `DATASET_PATH` below to match your dataset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATASET CONFIGURATION - AUTO-DETECTION\n# ============================================================\n\nimport os\nimport glob\n\n# Auto-detect dataset path\nprint(\"Searching for dataset...\")\nprint(\"=\"*60)\n\n# List all available datasets\navailable_datasets = os.listdir('/kaggle/input/') if os.path.exists('/kaggle/input/') else []\nprint(f\"Available datasets in /kaggle/input/:\")\nfor ds in available_datasets:\n    print(f\"  - {ds}\")\n\n# Try to find the dataset automatically\nDATASET_PATH = None\nIMAGE_DIR = None\nMASK_DIR = None\n\n# Check for common dataset structures\nfor dataset_name in available_datasets:\n    base_path = f\"/kaggle/input/{dataset_name}\"\n    \n    # Check for MACRO structure (your uploaded dataset)\n    if os.path.exists(f\"{base_path}/MACRO/raw_img\"):\n        DATASET_PATH = f\"{base_path}/MACRO\"\n        IMAGE_DIR = f\"{DATASET_PATH}/raw_img\"\n        MASK_DIR = None  # MACRO has TSV annotations, not masks\n        print(f\"\\nDetected MACRO dataset!\")\n        break\n    \n    # Check for MICRO structure\n    elif os.path.exists(f\"{base_path}/MICRO/raw_img\"):\n        DATASET_PATH = f\"{base_path}/MICRO\"\n        IMAGE_DIR = f\"{DATASET_PATH}/raw_img\"\n        MASK_DIR = f\"{DATASET_PATH}/annotation\"\n        print(f\"\\nDetected MICRO dataset!\")\n        break\n    \n    # Check for MESO structure\n    elif os.path.exists(f\"{base_path}/MESO/raw_img\"):\n        DATASET_PATH = f\"{base_path}/MESO\"\n        IMAGE_DIR = f\"{DATASET_PATH}/raw_img\"\n        MASK_DIR = f\"{DATASET_PATH}/annotation\"\n        print(f\"\\nDetected MESO dataset!\")\n        break\n    \n    # Check for simple images/masks structure\n    elif os.path.exists(f\"{base_path}/images\"):\n        DATASET_PATH = base_path\n        IMAGE_DIR = f\"{DATASET_PATH}/images\"\n        MASK_DIR = f\"{DATASET_PATH}/masks\" if os.path.exists(f\"{DATASET_PATH}/masks\") else None\n        print(f\"\\nDetected standard images/masks dataset!\")\n        break\n    \n    # Check for raw_img in root\n    elif os.path.exists(f\"{base_path}/raw_img\"):\n        DATASET_PATH = base_path\n        IMAGE_DIR = f\"{DATASET_PATH}/raw_img\"\n        MASK_DIR = None\n        print(f\"\\nDetected raw_img dataset!\")\n        break\n\n# Manual override option (uncomment and modify if auto-detection fails)\n# DATASET_PATH = \"/kaggle/input/your-dataset-name\"\n# IMAGE_DIR = f\"{DATASET_PATH}/MACRO/raw_img\"\n# MASK_DIR = None\n\n# ============================================================\n\n# Create output directories\nos.makedirs('/kaggle/working/models', exist_ok=True)\nos.makedirs('/kaggle/working/results', exist_ok=True)\n\n# Verify paths\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATASET CONFIGURATION\")\nprint(\"=\"*60)\n\nif DATASET_PATH is None or IMAGE_DIR is None:\n    print(\"\\n⚠️  ERROR: Could not auto-detect dataset!\")\n    print(\"\\nPlease set paths manually by uncommenting these lines above:\")\n    print(\"  DATASET_PATH = '/kaggle/input/your-dataset-name'\")\n    print(\"  IMAGE_DIR = f'{DATASET_PATH}/MACRO/raw_img'\")\n    print(\"\\nAvailable datasets:\", available_datasets)\nelse:\n    print(f\"\\n✓ Dataset path: {DATASET_PATH}\")\n    print(f\"✓ Image directory: {IMAGE_DIR}\")\n    print(f\"✓ Image directory exists: {os.path.exists(IMAGE_DIR)}\")\n    \n    if IMAGE_DIR and os.path.exists(IMAGE_DIR):\n        # Count images\n        image_count = 0\n        for ext in ['*.jpg', '*.jpeg', '*.JPG', '*.JPEG', '*.png', '*.PNG', '*.tif', '*.tiff']:\n            image_count += len(glob.glob(os.path.join(IMAGE_DIR, ext)))\n        \n        print(f\"✓ Total images found: {image_count}\")\n        \n        # Show first few images\n        all_images = []\n        for ext in ['*.jpg', '*.jpeg', '*.JPG', '*.JPEG', '*.png', '*.PNG']:\n            all_images.extend(glob.glob(os.path.join(IMAGE_DIR, ext)))\n        \n        if all_images:\n            print(f\"\\nFirst 5 images:\")\n            for img in sorted(all_images)[:5]:\n                print(f\"  - {os.path.basename(img)}\")\n        \n        # Check for masks\n        if MASK_DIR and os.path.exists(MASK_DIR):\n            print(f\"\\n✓ Mask directory: {MASK_DIR}\")\n            print(f\"✓ Masks exist: True\")\n        else:\n            print(f\"\\n⚠️  No mask directory found\")\n            print(\"  Will use dummy masks for training (demo mode)\")\n            print(\"  For real training, you need segmentation masks\")\n    else:\n        print(f\"\\n⚠️  ERROR: Image directory does not exist!\")\n        print(f\"  Path: {IMAGE_DIR}\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. U-Net Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Encoder\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Decoder\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = transforms.functional.resize(x, size=skip_connection.shape[2:])\n",
    "            \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Initialize model\n",
    "model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroplasticsDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir=None, transform=None, image_size=(256, 256)):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.images = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff']:\n",
    "            self.images.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "        \n",
    "        self.images = sorted(self.images)\n",
    "        print(f\"Found {len(self.images)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load mask if available\n",
    "        if self.mask_dir:\n",
    "            img_name = os.path.basename(img_path)\n",
    "            mask_path = os.path.join(self.mask_dir, img_name)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                mask = (mask > 0).astype(np.float32)\n",
    "            else:\n",
    "                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n",
    "        else:\n",
    "            # Create dummy mask for demo\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n",
    "        \n",
    "        # Resize\n",
    "        image = cv2.resize(image, self.image_size)\n",
    "        mask = cv2.resize(mask, self.image_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        return image, mask.unsqueeze(0)\n",
    "\n",
    "# Data transforms\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=45, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = MicroplasticsDataset(IMAGE_DIR, MASK_DIR if os.path.exists(MASK_DIR) else None, transform=train_transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "val_dataset.dataset.transform = val_transform\n",
    "test_dataset.dataset.transform = val_transform\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Data loaders\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Functions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2 * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice_loss = self.dice(pred, target)\n",
    "        return self.alpha * bce_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    pred_binary = (torch.sigmoid(pred) > threshold).float()\n",
    "    pred_np = pred_binary.cpu().numpy().flatten().astype(int)\n",
    "    target_np = target.cpu().numpy().flatten().astype(int)\n",
    "    \n",
    "    iou = jaccard_score(target_np, pred_np, zero_division=0)\n",
    "    f1 = f1_score(target_np, pred_np, zero_division=0)\n",
    "    precision = precision_score(target_np, pred_np, zero_division=0)\n",
    "    recall = recall_score(target_np, pred_np, zero_division=0)\n",
    "    \n",
    "    intersection = (pred_binary * target).sum()\n",
    "    dice = (2 * intersection) / (pred_binary.sum() + target.sum() + 1e-8)\n",
    "    \n",
    "    return {\n",
    "        'iou': iou,\n",
    "        'dice': dice.item(),\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "criterion = CombinedLoss(alpha=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'iou': 0, 'dice': 0, 'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    \n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        metrics = calculate_metrics(outputs, masks)\n",
    "        for key in total_metrics:\n",
    "            total_metrics[key] += metrics[key]\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_metrics = {key: val / len(dataloader) for key, val in total_metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'iou': 0, 'dice': 0, 'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            metrics = calculate_metrics(outputs, masks)\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += metrics[key]\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_metrics = {key: val / len(dataloader) for key, val in total_metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 20\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "train_metrics_history, val_metrics_history = [], []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_metrics_history.append(train_metrics)\n",
    "    \n",
    "    val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_metrics_history.append(val_metrics)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train IoU: {train_metrics['iou']:.4f} | Val IoU: {val_metrics['iou']:.4f}\")\n",
    "    print(f\"Train Dice: {train_metrics['dice']:.4f} | Val Dice: {val_metrics['dice']:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '/kaggle/working/models/best_unet_model.pth')\n",
    "        print(\"Best model saved!\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train')\n",
    "axes[0].plot(val_losses, label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "train_ious = [m['iou'] for m in train_metrics_history]\n",
    "val_ious = [m['iou'] for m in val_metrics_history]\n",
    "axes[1].plot(train_ious, label='Train')\n",
    "axes[1].plot(val_ious, label='Val')\n",
    "axes[1].set_title('IoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "train_dice = [m['dice'] for m in train_metrics_history]\n",
    "val_dice = [m['dice'] for m in val_metrics_history]\n",
    "axes[2].plot(train_dice, label='Train')\n",
    "axes[2].plot(val_dice, label='Val')\n",
    "axes[2].set_title('Dice')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/results/training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "def visualize_predictions(model, dataloader, num_samples=4):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(20, 12))\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    images, masks = next(data_iter)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images.to(device))\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        mask = masks[i].squeeze().numpy()\n",
    "        pred = predictions[i].squeeze().cpu().numpy()\n",
    "        \n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title(f'Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(mask, cmap='gray')\n",
    "        axes[1, i].set_title(f'Ground Truth {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        axes[2, i].imshow(pred, cmap='gray')\n",
    "        axes[2, i].set_title(f'Prediction {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/results/predictions.png')\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('/kaggle/working/models/best_unet_model.pth'))\n",
    "\n",
    "# Final evaluation\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loss, test_metrics = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test IoU: {test_metrics['iou']:.4f}\")\n",
    "print(f\"Test Dice: {test_metrics['dice']:.4f}\")\n",
    "print(f\"Test F1: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "\n",
    "print(\"\\nModel saved to: /kaggle/working/models/best_unet_model.pth\")\n",
    "print(\"Results saved to: /kaggle/working/results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}