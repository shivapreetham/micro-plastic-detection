{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7413272,"sourceType":"datasetVersion","datasetId":4312220}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ðŸ“¦ Import libraries\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.ops import box_iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T02:25:22.046305Z","iopub.execute_input":"2025-07-11T02:25:22.046491Z","iopub.status.idle":"2025-07-11T02:25:29.307267Z","shell.execute_reply.started":"2025-07-11T02:25:22.046473Z","shell.execute_reply":"2025-07-11T02:25:29.306644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ§¾ Dataset class\nclass MicroplasticDetectionDataset(Dataset):\n    def __init__(self, image_dir, annotation_file, transform=None):\n        self.image_dir = image_dir\n        self.annotations = pd.read_csv(annotation_file)\n        self.image_files = self.annotations[\"filename\"].unique()\n        self.transform = transform\n        self.class_dict = {\"Microplastic\": 1}\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        filename = self.image_files[idx]\n        image_path = os.path.join(self.image_dir, filename)\n        image = Image.open(image_path).convert(\"RGB\")\n\n        boxes = []\n        labels = []\n\n        records = self.annotations[self.annotations[\"filename\"] == filename]\n        for _, row in records.iterrows():\n            boxes.append([row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]])\n            labels.append(self.class_dict.get(row[\"class\"], 0))\n\n        target = {\n            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n            \"labels\": torch.tensor(labels, dtype=torch.int64),\n            \"image_id\": torch.tensor([idx])\n        }\n\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = ToTensor()(image)\n\n        return image, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T02:25:43.82115Z","iopub.execute_input":"2025-07-11T02:25:43.822052Z","iopub.status.idle":"2025-07-11T02:25:43.829831Z","shell.execute_reply.started":"2025-07-11T02:25:43.822017Z","shell.execute_reply":"2025-07-11T02:25:43.829077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“ Path configuration\ntrain_img_dir = \"/kaggle/input/microplastic-dataset-for-computer-vision/train\"\nvalid_img_dir = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid\"\ntrain_csv_path = os.path.join(train_img_dir, \"_annotations.csv\")\nvalid_csv_path = os.path.join(valid_img_dir, \"_annotations.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T02:25:46.851017Z","iopub.execute_input":"2025-07-11T02:25:46.851616Z","iopub.status.idle":"2025-07-11T02:25:46.855345Z","shell.execute_reply.started":"2025-07-11T02:25:46.851592Z","shell.execute_reply":"2025-07-11T02:25:46.854643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“š Dataset & Dataloader\ntrain_dataset = MicroplasticDetectionDataset(train_img_dir, train_csv_path)\nvalid_dataset = MicroplasticDetectionDataset(valid_img_dir, valid_csv_path)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T02:25:54.732915Z","iopub.execute_input":"2025-07-11T02:25:54.733552Z","iopub.status.idle":"2025-07-11T02:25:54.756103Z","shell.execute_reply.started":"2025-07-11T02:25:54.733531Z","shell.execute_reply":"2025-07-11T02:25:54.755542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ§  Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = fasterrcnn_resnet50_fpn(weights=None, weights_backbone=None, num_classes=2)\nmodel.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T02:25:58.065469Z","iopub.execute_input":"2025-07-11T02:25:58.065746Z","iopub.status.idle":"2025-07-11T02:25:59.062963Z","shell.execute_reply.started":"2025-07-11T02:25:58.065724Z","shell.execute_reply":"2025-07-11T02:25:59.061916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ” Training + validation loop\nnum_epochs = 50\nmodel.train()\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    print(f\"ðŸ§  Epoch {epoch + 1}/{num_epochs} - Train Loss: {epoch_loss:.4f}\")\n\n    # Validasi jumlah deteksi\n    def evaluate_on_validation(model, data_loader, device, score_thresh=0.5):\n        model.eval()\n        total_preds, total_gts = 0, 0\n        with torch.no_grad():\n            for images, targets in data_loader:\n                images = [img.to(device) for img in images]\n                outputs = model(images)\n                for output, target in zip(outputs, targets):\n                    preds = output[\"boxes\"][output[\"scores\"] > score_thresh]\n                    total_preds += len(preds)\n                    total_gts += len(target[\"boxes\"])\n        model.train()\n        return total_preds, total_gts\n\n    val_preds, val_gts = evaluate_on_validation(model, valid_loader, device)\n    print(f\"ðŸ” Validation - Predictions: {val_preds}, Ground Truths: {val_gts}\")\n\n    # Save model\n    torch.save(model.state_dict(), f\"fasterrcnn_microplastic_epoch{epoch+1}.pth\")\n    print(f\"âœ… Model saved: fasterrcnn_microplastic_epoch{epoch+1}.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T02:26:05.01498Z","iopub.execute_input":"2025-07-11T02:26:05.01531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Š Evaluasi detail per gambar\ndef compute_metrics_per_image(pred_boxes, gt_boxes, iou_thresh=0.5):\n    if len(pred_boxes) == 0 and len(gt_boxes) == 0:\n        return 1, 1, 1, 1\n    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n        return 0, 0, 0, 0\n\n    ious = box_iou(pred_boxes, gt_boxes)\n    matched = set()\n    tp = 0\n    for i in range(len(pred_boxes)):\n        max_iou = 0\n        match_idx = -1\n        for j in range(len(gt_boxes)):\n            if j in matched:\n                continue\n            if ious[i, j] > max_iou:\n                max_iou = ious[i, j]\n                match_idx = j\n        if max_iou >= iou_thresh:\n            tp += 1\n            matched.add(match_idx)\n\n    fp = len(pred_boxes) - tp\n    fn = len(gt_boxes) - tp\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n    mean_iou = ious.mean().item() if ious.numel() > 0 else 0\n    return precision, recall, f1, mean_iou\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:06:19.105345Z","iopub.execute_input":"2025-07-10T07:06:19.105788Z","iopub.status.idle":"2025-07-10T07:06:19.111956Z","shell.execute_reply.started":"2025-07-10T07:06:19.105766Z","shell.execute_reply":"2025-07-10T07:06:19.111162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ§¾ Load validation annotation\nval_annotations = pd.read_csv(valid_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:06:27.366834Z","iopub.execute_input":"2025-07-10T07:06:27.367409Z","iopub.status.idle":"2025-07-10T07:06:27.380534Z","shell.execute_reply.started":"2025-07-10T07:06:27.367386Z","shell.execute_reply":"2025-07-10T07:06:27.379855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“¦ Evaluasi model\nresults = []\nmodel.eval()\n\nfor fname in val_annotations[\"filename\"].unique():\n    image_path = os.path.join(valid_img_dir, fname)\n    image = Image.open(image_path).convert(\"RGB\")\n    img_tensor = ToTensor()(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        output = model(img_tensor)[0]\n\n    pred_boxes = output[\"boxes\"][output[\"scores\"] > 0.5].cpu()\n    pred_scores = output[\"scores\"][output[\"scores\"] > 0.5].cpu()\n    gt = val_annotations[val_annotations[\"filename\"] == fname]\n    gt_boxes = torch.tensor(gt[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values, dtype=torch.float)\n\n    precision, recall, f1, iou = compute_metrics_per_image(pred_boxes, gt_boxes)\n\n    results.append({\n        \"filename\": fname,\n        \"actual_count\": len(gt_boxes),\n        \"predicted_count\": len(pred_boxes),\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"mean_iou\": iou\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:06:30.065044Z","iopub.execute_input":"2025-07-10T07:06:30.06531Z","iopub.status.idle":"2025-07-10T07:06:41.726965Z","shell.execute_reply.started":"2025-07-10T07:06:30.06529Z","shell.execute_reply":"2025-07-10T07:06:41.726319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ’¾ Simpan hasil evaluasi\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"microplastic_detection_results.csv\", index=False)\nprint(\"ðŸ“ Evaluation results saved to: microplastic_detection_results.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:07:24.871671Z","iopub.execute_input":"2025-07-10T07:07:24.871984Z","iopub.status.idle":"2025-07-10T07:07:24.884027Z","shell.execute_reply.started":"2025-07-10T07:07:24.871963Z","shell.execute_reply":"2025-07-10T07:07:24.883311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“ˆ Rata-rata metrik keseluruhan\nprint(\"\\nðŸ“Š Summary of Detection Metrics:\")\nprint(f\"Average Precision: {results_df['precision'].mean():.4f}\")\nprint(f\"Average Recall:    {results_df['recall'].mean():.4f}\")\nprint(f\"Average F1 Score:  {results_df['f1_score'].mean():.4f}\")\nprint(f\"Average IoU:       {results_df['mean_iou'].mean():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:07:29.748851Z","iopub.execute_input":"2025-07-10T07:07:29.74939Z","iopub.status.idle":"2025-07-10T07:07:29.754421Z","shell.execute_reply.started":"2025-07-10T07:07:29.749367Z","shell.execute_reply":"2025-07-10T07:07:29.753713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T17:46:26.657517Z","iopub.execute_input":"2025-04-18T17:46:26.657794Z","iopub.status.idle":"2025-04-18T17:46:27.321306Z","shell.execute_reply.started":"2025-04-18T17:46:26.657767Z","shell.execute_reply":"2025-04-18T17:46:27.320498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom PIL import Image\nimport pandas as pd\nfrom torchvision.transforms import ToTensor\n\nclass MicroplasticCSVDetectionDataset(torch.utils.data.Dataset):\n    def __init__(self, img_dir, csv_path, transforms=None):\n        self.img_dir = img_dir\n        self.df = pd.read_csv(csv_path)\n        self.transforms = transforms\n        self.image_filenames = self.df[\"filename\"].unique()\n\n    def __getitem__(self, idx):\n        img_filename = self.image_filenames[idx]\n        img_path = os.path.join(self.img_dir, img_filename)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        # Get all rows in the CSV that match this image\n        records = self.df[self.df[\"filename\"] == img_filename]\n        \n        boxes = []\n        labels = []\n\n        for _, row in records.iterrows():\n            xmin = row[\"xmin\"]\n            ymin = row[\"ymin\"]\n            xmax = row[\"xmax\"]\n            ymax = row[\"ymax\"]\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(int(row[\"class\"]) + 1)  # Background is 0, microplastic is 1\n\n        target = {\n            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n            \"labels\": torch.tensor(labels, dtype=torch.int64),\n            \"image_id\": torch.tensor([idx])\n        }\n\n        if self.transforms:\n            img = self.transforms(img)\n        else:\n            img = ToTensor()(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_filenames)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T17:46:27.322819Z","iopub.execute_input":"2025-04-18T17:46:27.32331Z","iopub.status.idle":"2025-04-18T17:46:33.922347Z","shell.execute_reply.started":"2025-04-18T17:46:27.32329Z","shell.execute_reply":"2025-04-18T17:46:33.921759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Custom Dataset\nclass MicroplasticCSVDetectionDataset(Dataset):\n    def __init__(self, img_dir, csv_path, transforms=None):\n        self.img_dir = img_dir\n        self.df = pd.read_csv(csv_path)\n        self.transforms = transforms\n        self.image_filenames = self.df[\"filename\"].unique()\n        self.class_map = {\"Microplastic\": 1}  # Mapping string to int\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_filename = self.image_filenames[idx]\n        img_path = os.path.join(self.img_dir, img_filename)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        records = self.df[self.df[\"filename\"] == img_filename]\n\n        boxes = []\n        labels = []\n\n        for _, row in records.iterrows():\n            xmin = row[\"xmin\"]\n            ymin = row[\"ymin\"]\n            xmax = row[\"xmax\"]\n            ymax = row[\"ymax\"]\n            class_name = row[\"class\"]\n            class_id = self.class_map.get(class_name, 0)\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(class_id)\n\n        target = {\n            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n            \"labels\": torch.tensor(labels, dtype=torch.int64),\n            \"image_id\": torch.tensor([idx])\n        }\n\n        if self.transforms:\n            img = self.transforms(img)\n        else:\n            img = ToTensor()(img)\n\n        return img, target\n\n# Paths\ntrain_dir = \"/kaggle/input/microplastic-dataset-for-computer-vision/train\"\nvalid_dir = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid\"\ntrain_csv = os.path.join(train_dir, \"_annotations.csv\")\nvalid_csv = os.path.join(valid_dir, \"_annotations.csv\")\n\n# Datasets\ntrain_dataset = MicroplasticCSVDetectionDataset(train_dir, train_csv)\nvalid_dataset = MicroplasticCSVDetectionDataset(valid_dir, valid_csv)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\n# Model\nmodel = fasterrcnn_resnet50_fpn(num_classes=2)  # 1 class + background\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n\n# Training\nmodel.train()\nfor epoch in range(5):\n    total_loss = 0\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:55:09.521127Z","iopub.execute_input":"2025-04-18T19:55:09.521303Z","iopub.status.idle":"2025-04-18T20:01:09.708542Z","shell.execute_reply.started":"2025-04-18T19:55:09.521287Z","shell.execute_reply":"2025-04-18T20:01:09.707911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\n# Put model in eval mode\nmodel.eval()\n\n# Get a batch of validation data\nimages, targets = next(iter(valid_loader))\nimages = [img.to(device) for img in images]\n\n# Inference\nwith torch.no_grad():\n    predictions = model(images)\n\n# Visualize predictions for 1 image\nimage = images[0].cpu()\npred = predictions[0]\nboxes = pred['boxes'].cpu()\nscores = pred['scores'].cpu()\nlabels = pred['labels'].cpu()\n\n# Plot\nplt.figure(figsize=(8, 8))\nimg = F.to_pil_image(image)\nplt.imshow(img)\n\nfor box, score, label in zip(boxes, scores, labels):\n    if score > 0.5:  # Confidence threshold\n        x1, y1, x2, y2 = box\n        plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                          edgecolor='red', facecolor='none', linewidth=2))\n        plt.text(x1, y1, f'Microplastic: {score:.2f}', color='white',\n                 bbox=dict(facecolor='red', alpha=0.5))\n\nplt.axis('off')\nplt.title('Predictions')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:24:04.042585Z","iopub.execute_input":"2025-04-18T20:24:04.0429Z","iopub.status.idle":"2025-04-18T20:24:04.510697Z","shell.execute_reply.started":"2025-04-18T20:24:04.042876Z","shell.execute_reply":"2025-04-18T20:24:04.509817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"fasterrcnn_microplastic.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:37:27.060183Z","iopub.execute_input":"2025-04-18T20:37:27.060464Z","iopub.status.idle":"2025-04-18T20:37:27.329941Z","shell.execute_reply.started":"2025-04-18T20:37:27.060439Z","shell.execute_reply":"2025-04-18T20:37:27.3292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"fasterrcnn_microplastic.pth\"))\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T17:52:25.440176Z","iopub.execute_input":"2025-04-18T17:52:25.440871Z","iopub.status.idle":"2025-04-18T17:52:25.611442Z","shell.execute_reply.started":"2025-04-18T17:52:25.440843Z","shell.execute_reply":"2025-04-18T17:52:25.610719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef predict_image(image_path):\n    model.eval()\n    img = Image.open(image_path).convert(\"RGB\")\n    img_tensor = ToTensor()(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        output = model(img_tensor)[0]\n\n    return output\n\noutput = predict_image(\"/kaggle/input/microplastic-dataset-for-computer-vision/valid/a--3-_jpg.rf.8248ba99e3b3ae254d1723b674f7fd99.jpg\")\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:02:47.468593Z","iopub.execute_input":"2025-04-18T21:02:47.468896Z","iopub.status.idle":"2025-04-18T21:02:47.548535Z","shell.execute_reply.started":"2025-04-18T21:02:47.468872Z","shell.execute_reply":"2025-04-18T21:02:47.547905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nimport torch\n\n# Load model\nmodel = fasterrcnn_resnet50_fpn(num_classes=2)\nmodel.load_state_dict(torch.load(\"fasterrcnn_microplastic.pth\"))  # Update path if needed\nmodel.eval()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:04:27.928174Z","iopub.execute_input":"2025-04-18T21:04:27.928449Z","iopub.status.idle":"2025-04-18T21:04:28.797019Z","shell.execute_reply.started":"2025-04-18T21:04:27.928426Z","shell.execute_reply":"2025-04-18T21:04:28.796388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.transforms import ToTensor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef predict_and_plot(image_path, threshold=0.5):\n    # Load and preprocess image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = ToTensor()(image).unsqueeze(0).to(device)\n\n    # Run prediction\n    with torch.no_grad():\n        outputs = model(image_tensor)\n\n    outputs = outputs[0]\n    boxes = outputs['boxes'].cpu()\n    scores = outputs['scores'].cpu()\n    labels = outputs['labels'].cpu()\n\n    # Plot\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    ax = plt.gca()\n\n    for box, score in zip(boxes, scores):\n        if score > threshold:\n            x1, y1, x2, y2 = box\n            ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                       linewidth=2, edgecolor='lime', facecolor='none'))\n            ax.text(x1, y1, f'{score:.2f}', color='white',\n                    bbox=dict(facecolor='lime', edgecolor='none', alpha=0.7))\n\n    plt.axis(\"off\")\n    plt.title(\"Predicted Microplastics\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:04:38.441971Z","iopub.execute_input":"2025-04-18T21:04:38.442231Z","iopub.status.idle":"2025-04-18T21:04:38.449039Z","shell.execute_reply.started":"2025-04-18T21:04:38.442209Z","shell.execute_reply":"2025-04-18T21:04:38.448232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_and_plot(\"/kaggle/input/microplastic-dataset-for-computer-vision/valid/a--23-_jpg.rf.1ab5e302030f3bb3c08981ca42a8e631.jpg\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:04:57.419371Z","iopub.execute_input":"2025-04-18T21:04:57.419962Z","iopub.status.idle":"2025-04-18T21:04:57.819928Z","shell.execute_reply.started":"2025-04-18T21:04:57.419937Z","shell.execute_reply":"2025-04-18T21:04:57.819281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport torch\nfrom torchvision.transforms import ToTensor\n\ndef visualize_prediction_vs_actual(image_path, annotation_df, model, threshold=0.5):\n    # Extract filename\n    filename = image_path.split(\"/\")[-1]\n\n    # Load image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = ToTensor()(image).unsqueeze(0).to(device)\n\n    # Predict with model\n    model.eval()\n    with torch.no_grad():\n        prediction = model(image_tensor)[0]\n\n    pred_boxes = prediction['boxes'].cpu()\n    pred_scores = prediction['scores'].cpu()\n    pred_boxes = pred_boxes[pred_scores > threshold]\n\n    # Get ground truth from CSV\n    gt_boxes = annotation_df[annotation_df[\"filename\"] == filename]\n    \n    # Plotting\n    fig, ax = plt.subplots(1, figsize=(12, 12))\n    ax.imshow(image)\n\n    # Draw ground truth (green)\n    for _, row in gt_boxes.iterrows():\n        xmin, ymin, xmax, ymax = row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]\n        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                 linewidth=2, edgecolor='lime', facecolor='none')\n        ax.add_patch(rect)\n\n    # Draw predictions (red)\n    for box in pred_boxes:\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                 linewidth=2, edgecolor='red', facecolor='none')\n        ax.add_patch(rect)\n\n    # Count\n    actual_count = len(gt_boxes)\n    predicted_count = len(pred_boxes)\n\n    plt.title(f\"Actual: {actual_count} vs Predicted: {predicted_count}\", fontsize=16)\n    plt.axis(\"off\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:07:36.461843Z","iopub.execute_input":"2025-04-18T21:07:36.462437Z","iopub.status.idle":"2025-04-18T21:07:36.474713Z","shell.execute_reply.started":"2025-04-18T21:07:36.462409Z","shell.execute_reply":"2025-04-18T21:07:36.473814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: visualize for one image from validation set\nimage_path = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid/a--46-_jpg.rf.f2fc88b781d9457f65bfee1a29f6ca49.jpg\"\nannotation_csv = pd.read_csv(\"/kaggle/input/microplastic-dataset-for-computer-vision/valid/_annotations.csv\")\n\nvisualize_prediction_vs_actual(image_path, annotation_csv, model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:13:14.713402Z","iopub.execute_input":"2025-04-18T21:13:14.713983Z","iopub.status.idle":"2025-04-18T21:13:15.207107Z","shell.execute_reply.started":"2025-04-18T21:13:14.713958Z","shell.execute_reply":"2025-04-18T21:13:15.206177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision.transforms import ToTensor\nfrom torchvision.ops import box_iou\n\n# Load your annotation CSV\nvalid_csv_path = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid/_annotations.csv\"\nannotation_df = pd.read_csv(valid_csv_path)\n\n# Folder where your images are\nvalid_images_dir = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid\"\n\n# Get unique image filenames\nimage_files = annotation_df[\"filename\"].unique()\n\n# Evaluation function\ndef evaluate_detection(pred_boxes, gt_boxes, iou_threshold=0.5):\n    if len(pred_boxes) == 0 and len(gt_boxes) == 0:\n        return 1, 1, 1, 1\n    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n        return 0, 0, 0, 0\n\n    ious = box_iou(pred_boxes, gt_boxes)\n    matched_gt = set()\n    tp = 0\n\n    for i in range(len(pred_boxes)):\n        max_iou = 0\n        max_j = -1\n        for j in range(len(gt_boxes)):\n            if j in matched_gt:\n                continue\n            if ious[i, j] > max_iou:\n                max_iou = ious[i, j]\n                max_j = j\n        if max_iou >= iou_threshold:\n            tp += 1\n            matched_gt.add(max_j)\n\n    fp = len(pred_boxes) - tp\n    fn = len(gt_boxes) - tp\n\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n    mean_iou = ious.mean().item() if ious.numel() > 0 else 0\n    return precision, recall, f1, mean_iou\n\n# Run evaluation on all images\nresults = []\nmodel.eval()\n\nfor filename in image_files:\n    img_path = os.path.join(valid_images_dir, filename)\n    image = Image.open(img_path).convert(\"RGB\")\n    img_tensor = ToTensor()(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        prediction = model(img_tensor)[0]\n\n    # Get predicted boxes\n    pred_boxes = prediction['boxes'][prediction['scores'] > 0.5].cpu()\n\n    # Get ground truth boxes for that image\n    gt = annotation_df[annotation_df[\"filename\"] == filename]\n    gt_boxes = torch.tensor(gt[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values, dtype=torch.float)\n\n    precision, recall, f1, iou = evaluate_detection(pred_boxes, gt_boxes)\n\n    results.append({\n        \"filename\": filename,\n        \"actual_count\": len(gt_boxes),\n        \"predicted_count\": len(pred_boxes),\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"mean_iou\": iou\n    })\n\n# Save results to CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"microplastic_detection_metrics.csv\", index=False)\nprint(\"Evaluation saved to microplastic_detection_metrics.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:15:19.600656Z","iopub.execute_input":"2025-04-18T21:15:19.601246Z","iopub.status.idle":"2025-04-18T21:15:33.38727Z","shell.execute_reply.started":"2025-04-18T21:15:19.601222Z","shell.execute_reply":"2025-04-18T21:15:33.386563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision.transforms import ToTensor\nfrom torchvision.ops import box_iou\n\n# Path to validation data\nvalid_csv_path = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid/_annotations.csv\"\nvalid_images_dir = \"/kaggle/input/microplastic-dataset-for-computer-vision/valid\"\nannotation_df = pd.read_csv(valid_csv_path)\n\nimage_files = annotation_df[\"filename\"].unique()\n\n# Setup\nmodel.eval()\nall_pred_boxes = []\nall_gt_boxes = []\n\n# Collect predictions and GT boxes from all images\nfor filename in image_files:\n    img_path = os.path.join(valid_images_dir, filename)\n    image = Image.open(img_path).convert(\"RGB\")\n    img_tensor = ToTensor()(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        prediction = model(img_tensor)[0]\n\n    # Filter predictions with confidence > 0.5\n    pred_boxes = prediction['boxes'][prediction['scores'] > 0.5].cpu()\n    all_pred_boxes.extend(pred_boxes)\n\n    # Ground truth boxes\n    gt = annotation_df[annotation_df[\"filename\"] == filename]\n    gt_boxes = torch.tensor(gt[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values, dtype=torch.float)\n    all_gt_boxes.extend(gt_boxes)\n\n# Convert to tensors\nall_pred_boxes = torch.stack(all_pred_boxes) if all_pred_boxes else torch.empty((0, 4))\nall_gt_boxes = torch.stack(all_gt_boxes) if all_gt_boxes else torch.empty((0, 4))\n\n# Evaluation function\ndef evaluate_all(pred_boxes, gt_boxes, iou_threshold=0.5):\n    if len(pred_boxes) == 0 and len(gt_boxes) == 0:\n        return 1, 1, 1, 1\n    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n        return 0, 0, 0, 0\n\n    ious = box_iou(pred_boxes, gt_boxes)\n    matched_gt = set()\n    tp = 0\n\n    for i in range(len(pred_boxes)):\n        max_iou = 0\n        max_j = -1\n        for j in range(len(gt_boxes)):\n            if j in matched_gt:\n                continue\n            if ious[i, j] > max_iou:\n                max_iou = ious[i, j]\n                max_j = j\n        if max_iou >= iou_threshold:\n            tp += 1\n            matched_gt.add(max_j)\n\n    fp = len(pred_boxes) - tp\n    fn = len(gt_boxes) - tp\n\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n    mean_iou = ious.mean().item() if ious.numel() > 0 else 0\n    return precision, recall, f1, mean_iou\n\n# Compute overall metrics\nprecision, recall, f1, mean_iou = evaluate_all(all_pred_boxes, all_gt_boxes)\n\n# Show results\nprint(f\"ðŸ“ˆ Overall Validation Metrics:\")\nprint(f\"   Precision: {precision:.4f}\")\nprint(f\"   Recall:    {recall:.4f}\")\nprint(f\"   F1 Score:  {f1:.4f}\")\nprint(f\"   Mean IoU:  {mean_iou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:18:54.384306Z","iopub.execute_input":"2025-04-18T21:18:54.384913Z","iopub.status.idle":"2025-04-18T21:19:23.4495Z","shell.execute_reply.started":"2025-04-18T21:18:54.384888Z","shell.execute_reply":"2025-04-18T21:19:23.448815Z"}},"outputs":[],"execution_count":null}]}